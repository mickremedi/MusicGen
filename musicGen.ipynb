{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "class LSTMnn(nn.Module):\n",
    "    def __init__(self, num_categories, hidden_size, num_layers, dropout = 0):\n",
    "        super(LSTMnn, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_categories = num_categories\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(num_categories, hidden_size, num_layers, dropout = dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_categories)\n",
    "    \n",
    "    def forward(self, batch, hidden_state):\n",
    "        outputs, hidden_state = self.lstm(batch, hidden_state)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# Maps each music note in 'filename' to an integer\n",
    "def createMap(filename):\n",
    "    char2int = dict()\n",
    "    int2char = dict()\n",
    "    i = 1\n",
    "    with open(filename) as f:\n",
    "        while True:\n",
    "            c = f.read(1)\n",
    "            if not c:\n",
    "                break\n",
    "            if c not in char2int:\n",
    "                char2int[c] = i\n",
    "                int2char[i] = c\n",
    "                i += 1\n",
    "    return char2int, int2char\n",
    "\n",
    "# One-hot encodes song using previously created mapping from notes to integers\n",
    "def encodeSong(songString, char2int):\n",
    "    encoding = torch.zeros((len(songString), len(char2int) + 1))\n",
    "    for i in range(len(songString)):\n",
    "        if songString[i] in char2int:\n",
    "            ind = char2int[songString[i]]\n",
    "        else:\n",
    "            ind = 0\n",
    "        encoding[i][ind] = 1\n",
    "    return encoding\n",
    "\n",
    "# One-hot encodes all songs in 'filename' and returns list of encoded songs\n",
    "def encodeFile(filename, char2int):\n",
    "    songs = []\n",
    "    curSong = \"\"\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            curSong += line\n",
    "            if line == \"<end>\\n\":\n",
    "                songs.append(encodeSong(curSong, char2int))\n",
    "                curSong = \"\"\n",
    "    return songs\n",
    "\n",
    "# Converts one-hot encoded song back into music notes using mapping from integers to notes\n",
    "def decodeSong(rawEncoding, int2char):\n",
    "    encoding = rawEncoding.reshape((-1, len(int2char) + 1))\n",
    "    song = \"\"\n",
    "    for e in encoding:\n",
    "        if np.argmax(e) in int2char:\n",
    "            song +=  int2char[np.argmax(e)]\n",
    "    return song\n",
    "\n",
    "def decodeInts(ints, int2char, song = \"\"):\n",
    "    for i in ints:\n",
    "        if i in int2char:\n",
    "            song += int2char[i]\n",
    "    return song\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU available...\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of one-hot encoding of songs\n",
    "char2int, int2char = createMap(\"train.txt\")\n",
    "songs = encodeFile(\"train.txt\", char2int)\n",
    "val_songs = encodeFile(\"val.txt\", char2int)\n",
    "\n",
    "# Input includes each valid note value plus an extra mapping for any unknown characters\n",
    "input_size  = len(char2int) + 1\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "\n",
    "# Training variables\n",
    "epochs = 100\n",
    "notes_per_batch = 100\n",
    "learning_rate = .003\n",
    "\n",
    "# Creates network, backprop system with learning rate of 0.001, and cross entropy loss criterion\n",
    "lstm = LSTMnn(input_size, hidden_size, num_layers).to(device)\n",
    "optimizer = optim.Adam(lstm.parameters(),lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "Training song #100\n",
      "Training song #200\n",
      "Training song #300\n",
      "Training song #400\n",
      "Training song #500\n",
      "Training song #600\n",
      "Training song #700\n",
      "Training song #800\n",
      "training loss:  2.198507622169787\n",
      "1.9763635334215666\n",
      "epoch  1\n",
      "Training song #100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training epochs\n",
    "vlosses = []\n",
    "tlosses = []\n",
    "early_stop = 1\n",
    "for e in range(epochs):\n",
    "    print(\"epoch \", e)\n",
    "\n",
    "    # Shuffles training data\n",
    "    indices = list(range(len(songs)))\n",
    "    random.shuffle(indices)\n",
    "    tloss = []\n",
    "    \n",
    "    # Loop through training data\n",
    "    for songNum, i in enumerate(indices):\n",
    "        if songNum % 100 == 99:\n",
    "            print(\"Training song #%d\" % (songNum + 1))\n",
    "        \n",
    "        # Initializes the hidden state = (short term, long term) and the current song\n",
    "        hidden_state = (torch.zeros((num_layers, 1, hidden_size)).to(device), torch.zeros((num_layers, 1, hidden_size)).to(device))\n",
    "        song = songs[i]\n",
    "\n",
    "        # Loops through song 100 notes at a time\n",
    "        for j in range(math.ceil(len(song) / notes_per_batch)):\n",
    "            lstm.zero_grad()\n",
    "\n",
    "            # Creates a batch of 100 notes, reshaping to give an extra dimension\n",
    "            batch = song[j * notes_per_batch : min((j + 1) * notes_per_batch, len(song) - 1)]\n",
    "            batch = batch.reshape( (-1, 1, input_size)).to(device)\n",
    "\n",
    "            # Creates a batch of the 100 notes the model should return, which is the note after the current note\n",
    "            teacher = song[j * notes_per_batch + 1 : min((j + 1) * notes_per_batch + 1, len(song))]\n",
    "            if len(teacher) == 0:\n",
    "                continue\n",
    "            teacher = torch.argmax(teacher, 1).to(device)\n",
    "            \n",
    "            # Calls the lstm model with the current batch and hidden state\n",
    "            outputs, hidden_state = lstm(batch, (hidden_state[0].detach(), hidden_state[1].detach()))\n",
    "\n",
    "            # Performs backprop\n",
    "            loss = criterion(outputs.reshape(-1, input_size), teacher)\n",
    "            tloss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    tlosses.append(sum(tloss) / len(tloss))\n",
    "    print(\"training loss: \", sum(tloss) / len(tloss))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for song in val_songs:\n",
    "            hidden_state = (torch.zeros((num_layers, 1, hidden_size)).to(device), torch.zeros((num_layers, 1, hidden_size)).to(device))\n",
    "            \n",
    "            teacher = song[1:]\n",
    "\n",
    "            teacher = torch.argmax(teacher, 1).to(device)\n",
    "\n",
    "            song = song.reshape(-1, 1, input_size).to(device)\n",
    "\n",
    "            output, hidden_state = lstm(song, hidden_state)\n",
    "\n",
    "            output = output.reshape(-1, input_size)\n",
    "\n",
    "            loss += criterion(output[:len(output) - 1], teacher)\n",
    "\n",
    "        print(loss.item() / len(val_songs))\n",
    "        vlosses.append(loss.item() / len(val_songs))\n",
    "        if e >= early_stop:\n",
    "            stop = True\n",
    "            for i in range(early_stop):\n",
    "                if vlosses[e - i] < vlosses[e - i - 1]:\n",
    "                    stop = False\n",
    "\n",
    "            if stop:\n",
    "                break\n",
    "torch.save(lstm.state_dict(), \"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "X:11\n",
      "T:La toushersous, see #104\n",
      "D:Noel Mazurka a she perg?e\n",
      "R:Bourre? de la partsoy\n",
      "Z:id:hn-hornpipe-8\n",
      "M:C|\n",
      "K:G\n",
      "(3ABc|AFDF G2EF|GFGB AGFA|BGGF G2:|\n",
      "<end>\n",
      "---\n",
      "<start>\n",
      "X:53\n",
      "T:Branle\n",
      "R:Branslanome larousion\n",
      "C:Trad.\n",
      "S:Thoinot Arbeau\n",
      "N:0175.gif\n",
      "B:Orchesographie (1589)\n",
      "O:France\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2006-11-23\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "M:2/4\n",
      "L:1/8\n",
      "Q:1/4=120\n",
      "R:Transcrit et/ou corrig? par Michel BELLON - 2007-04-03\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "M:2/4\n",
      "L:1/8\n",
      "Q:1/4=120\n",
      "K:Bb\n",
      "V:Farone\n",
      "AA Ge de | dc BA GB | c2 e2 de | f2 gg b2 | g2 g2 f2 | e2 fe d2 | f2 ed cA | B2 B2 A2 | G4 || \n",
      "<end>\n",
      "---\n",
      "<start>\n",
      "X:83\n",
      "T:Ro?requetedufetso de la bauge\n",
      "O:France\n",
      "A:Provence\n",
      "C:ariestias Hornpipe\n",
      "C:James Hornpipe\", veel~\n",
      "Z:id:hn-hornpipe-89\n",
      "M:C|\n",
      "K:Ador\n",
      "cA | cAFA GABG | AGAB cdef | gddf gedc |\n",
      "B2Bd BAGF | A2G2 G2BA | AGAB cBAF | E2EE D2GF | FGAF E2G2 | G2G2 G2 :|\n",
      "<end>\n",
      "---\n",
      "<start>\n",
      "X:49\n",
      "T:Pad?ron ? Callave\n",
      "R:Schottische\n",
      "Q:1/4=105\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-16\n",
      "M:C|\n",
      "L:1/8\n",
      "K:G\n",
      "B2 GA|c2 d2|ee d2|d2 d2|c2 de|fe dc|BG F2|D2 DF|ED E2:|\n",
      "|:A2 e2|e2 fe|de fd|ed d2|c>B AG|FA AB|cA AG|BA FE|DE E2:|\n",
      "<end>\n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-933338d941ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mcurchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mcurchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mcurchar\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mTemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcurchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##SONG GENERATION\n",
    "import re\n",
    "regex = re.compile('\".\"')\n",
    "start_string = '<start>\\n'\n",
    "Temperature = .7\n",
    "softmax = nn.Softmax()\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        hidden_state = (torch.zeros((num_layers, 1, hidden_size)).to(device), torch.zeros((num_layers, 1, hidden_size)).to(device))\n",
    "        encoding = encodeSong(start_string, char2int)\n",
    "        curchar = torch.tensor(encoding).to(device)\n",
    "        encoding = [torch.argmax(e).item() for e in encoding]\n",
    "        song = start_string\n",
    "        while \"<end>\" not in song:\n",
    "            curchar = curchar.reshape(-1, 1, input_size).to(device)\n",
    "            outputs, hidden_state = lstm(curchar, hidden_state)\n",
    "            curchar = outputs[len(outputs) - 1][0]\n",
    "            curchar /= Temperature\n",
    "            curchar = softmax(curchar).cpu().numpy()\n",
    "            guess = np.random.choice(np.arange(input_size), p = curchar)\n",
    "            curchar = torch.zeros((input_size)).to(device)\n",
    "            curchar[guess] = 1\n",
    "            if guess in int2char:\n",
    "                song += int2char[guess]\n",
    "        print(song)\n",
    "        print(\"---\")\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(vlosses[len(vlosses) - 1])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(vlosses)), np.array(vlosses), label = \"Validation Loss\")\n",
    "plt.plot(np.arange(len(tlosses)), np.array(tlosses), label = \"Training Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss with \" + str(num_layers) + \" \" + str(hidden_size) + \" node layer, \" + str(learning_rate) + \" learning rate, \" + str(notes_per_batch) + \" character chunk\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HEATMAP GENERATION\n",
    "import seaborn as sb\n",
    "\n",
    "# Initializes the hidden state = (short term, long term) and the current song\n",
    "hidden_state = (torch.zeros((num_layers, 1, hidden_size)).to(device), torch.zeros((num_layers, 1, hidden_size)).to(device))\n",
    "song = songs[0]\n",
    "outs = []\n",
    "labs = []\n",
    "with torch.no_grad():\n",
    "    # Loops through song 100 notes at a time\n",
    "    for j in range(len(song) - 1):\n",
    "        inp = song[j]\n",
    "        inp = inp.reshape(1, 1, input_size).to(device)\n",
    "        output, hidden_state = lstm(inp, hidden_state)\n",
    "        lab = torch.argmax(song[j + 1], 0).to(device)\n",
    "        \n",
    "        labs.append(int2char[lab.item()])\n",
    "        outs.append(hidden_state[0][0][0][150].item())\n",
    "        \n",
    "outs = np.array(outs)\n",
    "labs = np.array(labs)\n",
    "outs = outs.reshape(34,-1)\n",
    "labs = labs.reshape(34,-1)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "heat_map = sb.heatmap(outs, annot=labs, fmt='', cmap=\"YlOrRd\", square=False)\n",
    "bottom, top = heat_map.get_ylim()\n",
    "heat_map.set_ylim(bottom + 0.5, top - 0.5)\n",
    "heat_map.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the hidden state = (short term, long term) and the current song\n",
    "hidden_state = (torch.zeros((num_layers, 1, hidden_size)).to(device), torch.zeros((num_layers, 1, hidden_size)).to(device))\n",
    "song = songs[i]\n",
    "outs = []\n",
    "labs = []\n",
    "tsongs = encodeFile(\"test.txt\", char2int)\n",
    "right = 0\n",
    "m_loss = 0\n",
    "wrong = 0\n",
    "with torch.no_grad():\n",
    "    # Loops through song 100 notes at a time\n",
    "    for i in range(len(tsongs)):\n",
    "        song = tsongs[i]\n",
    "        hidden_state = (torch.zeros((num_layers, 1, hidden_size)).to(device), torch.zeros((num_layers, 1, hidden_size)).to(device))\n",
    "            \n",
    "        teacher = song[1:]\n",
    "\n",
    "        teacher = torch.argmax(teacher, 1).to(device)\n",
    "\n",
    "        song = song.reshape(-1, 1, input_size).to(device)\n",
    "\n",
    "        output, hidden_state = lstm(song, hidden_state)\n",
    "\n",
    "        output = output.reshape(-1, input_size)\n",
    "\n",
    "        m_loss += criterion(output[:len(output) - 1], teacher)\n",
    "print(m_loss / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
